{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Distance explainer quick start\n",
    "\n",
    "In this notebook we show how one typically would use the distance_explainer package. We show how to explain how a model embeds two images into an embedded space."
   ],
   "id": "29ec9b04e9ad8939"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-05T11:46:00.775102Z",
     "start_time": "2024-11-05T11:45:58.898168Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from distance_explainer import DistanceExplainer"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's assume we have some model that embeds an input image and into some embedded space. This model can be a neural net or deep network of any kind. For the purpose of this tutorial we are using a dummy model that outputs a random embedding.",
   "id": "fa4513a99368ad79"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T11:34:22.011223Z",
     "start_time": "2024-11-05T11:34:22.006394Z"
    }
   },
   "cell_type": "code",
   "source": [
    "DUMMY_EMBEDDING_DIMENSIONALITY = 10\n",
    "dummy_model = lambda x: np.random.randn(x.shape[0], DUMMY_EMBEDDING_DIMENSIONALITY)"
   ],
   "id": "987c7436a16f7c6f",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We now have 2 images whose relationship we want to understand in the embedded space. In other words, we want to understand how the model embeds these two images relative to each other.\n",
    "\n",
    "Note that we are using images with 3 channels (RGB) that have their channels on axis 2."
   ],
   "id": "3f529417f4c1ab92"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "image1 = np.random.random((100, 100, 3))\n",
    "image2 = np.random.random((100, 100, 3))\n",
    "\n",
    "axis_labels = {2: 'channels'}"
   ],
   "id": "ca97aa48e6fde010",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We are going to embed the second image, and use its position in the embedded space as a reference point to compare to.\n",
    "We are then going to perturb our first image, by masking some of its features, and then embed the masked image, many times. This results in a list of points in the embedded space, representing each masked image. We then calculate the distance of those embedded points to the reference point representing image2. Finally, we combine the masks with the resulting distances to form a attribution map."
   ],
   "id": "391b659badf72458"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-05T11:40:28.586Z",
     "start_time": "2024-11-05T11:40:26.972815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batch = image2[None, ...]  # We create a batch of 1 because we expect the model to run on batches\n",
    "embedded_batch = dummy_model(batch)\n",
    "image2_embedded = embedded_batch[0]\n",
    "\n",
    "explainer = DistanceExplainer(axis_labels=axis_labels)  # Make a distance explainer object, and set the channel axis.\n",
    "attribution_map = explainer.explain_image_distance(dummy_model, image1, image2_embedded)[0]  # Multiple maps can be returned, we take the first (and in this case the only) map"
   ],
   "id": "fd69230c0eee80d1",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Explaining: 100%|██████████| 100/100 [00:00<00:00, 248.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ref.shape=(1, 10)\n",
      "ref.shape=(1, 10)\n",
      "(100, 100, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The attribution_map shows us which parts of image1 brings it closer to image2 in the embedded space, and what parts bring it further away. The map can easily be visualized like any other greyscale image. Because we used random data and model, we don't visualize the attribution map in this notebook.",
   "id": "947913431a9cfbbd"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
